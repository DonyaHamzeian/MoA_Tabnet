{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General ###\n",
    "import os\n",
    "import copy\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "\n",
    "### Data Wrangling ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "### Machine Learning ###\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Deep Learning ###\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Tabnet \n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pickle import load,dump\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "### Make prettier the prints ###\n",
    "from colorama import Fore\n",
    "c_ = Fore.CYAN\n",
    "m_ = Fore.MAGENTA\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "y_ = Fore.YELLOW\n",
    "g_ = Fore.GREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_targets_scored = pd.read_csv('train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We used nonscored MoA's to further train our model. However, 208 of them had less than 10 positive labels, so we removed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "# if a column in nonscored has less than 10 ones, drop it\n",
    "count = 0\n",
    "small = []\n",
    "for col in range(len(train_targets_nonscored.columns)):\n",
    "    if col==0:\n",
    "        continue\n",
    "    if sum(train_targets_nonscored.iloc[:, col])<10:\n",
    "        small.append(col)\n",
    "        count+=1\n",
    "            \n",
    "print(count)\n",
    "train_targets_nonscored = train_targets_nonscored.drop(train_targets_nonscored.columns[small], axis = 1).reset_index(drop = True)\n",
    "# 208 columns were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate genes and cells columns\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything\n",
    "seed = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster gene columns to get the (gene) pseudo labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster_genes(train, test, n_clusters_g = 20, SEED = 42):\n",
    "    \n",
    "    features_g = GENES\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans_genes = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "        dump(kmeans_genes, open('kmeans_genes.pkl', 'wb'))\n",
    "        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n",
    "        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    return train, test\n",
    "\n",
    "train_features ,test_features = fe_cluster_genes(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster cell columns to get the (cell) pseudo labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_cluster_cells(train, test, n_clusters_c = 5, SEED = 42):\n",
    "    \n",
    "    features_c = CELLS\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans_cells = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "        dump(kmeans_cells, open('kmeans_cells.pkl', 'wb'))\n",
    "        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n",
    "        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    return train, test\n",
    "\n",
    "train_features, test_features = fe_cluster_cells(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop control rows from the train set since these samples did not receive any drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train.merge(train_targets_scored, on='sig_id')\n",
    "\n",
    "ctrl = train[train['cp_type']=='ctl_vehicle'].reset_index(drop=True)\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data: we used the columns from the control data for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "num_samples = int(len(train)*np.floor(c))\n",
    "sample_train = train.sample(num_samples, replace = False).reset_index(drop = True)\n",
    "sample_ctrl = ctrl.sample(num_samples, replace = True).reset_index(drop = True)\n",
    "\n",
    "sample_train[GENES+CELLS] += sample_ctrl[GENES+CELLS]\n",
    "train = pd.concat([train, sample_train], axis = 0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "target_step1 = train[list(train_targets_nonscored.columns) +list(train_targets_scored.columns)]\n",
    "\n",
    "target_step1=target_step1.drop('sig_id', axis = 1)\n",
    "# 400 columns will be in target_step 1 where the first part is related to the nonscored columns\n",
    "target_step2 = train[train_targets_scored.columns]\n",
    "target_step2=target_step2.drop('sig_id', axis = 1)\n",
    "#target_step_2 has 206 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dose can be D1 or D2. cp_time can be 24, 48, or 72\n",
    "train = pd.get_dummies(train, columns=['cp_time','cp_dose'])\n",
    "test_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We trained the model in 2 steps. First to predict non-scored MoA's for the test set then to use test features + test non-scored MoA's for predicting the scored MoA's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_step1 = [c for c in train.columns if c in train_features.columns and c not in ['sig_id']]\n",
    "feature_cols_step2 = [c for c in train.columns if (c in train_features.columns or c in train_targets_nonscored.columns)and c not in ['sig_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step1 = train[feature_cols_step1]\n",
    "train_step2 = train[feature_cols_step2]\n",
    "test = test_[feature_cols_step1]\n",
    "X_test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score in Kaggle is calculated based on the LogitsLogLoss\n",
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 5e-5) + y_true * np.log(logits + 5e-5)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best TabNet hyperparameters found by tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 100\n",
    "\n",
    "best_tabnet_params = dict(\n",
    "    n_d = 32,\n",
    "    n_a = 32,\n",
    "    n_steps = 3,\n",
    "    gamma = 2,\n",
    "    lambda_sparse = 0.000001,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = seed,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.14261 |  0:00:08s\n",
      "epoch 10 | loss: 0.01425 |  0:01:33s\n",
      "epoch 20 | loss: 0.01362 |  0:02:57s\n",
      "epoch 30 | loss: 0.0131  |  0:04:23s\n",
      "epoch 40 | loss: 0.01262 |  0:05:48s\n",
      "epoch 50 | loss: 0.01239 |  0:07:14s\n",
      "epoch 60 | loss: 0.01225 |  0:08:40s\n",
      "epoch 70 | loss: 0.01216 |  0:10:05s\n",
      "epoch 80 | loss: 0.01219 |  0:11:30s\n",
      "epoch 90 | loss: 0.01216 |  0:12:55s\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train_step1.values, target_step1.values\n",
    "model = TabNetRegressor(**best_tabnet_params)\n",
    "model.fit(\n",
    "        X_train = X_train,\n",
    "        y_train = y_train,\n",
    "        max_epochs = MAX_EPOCH,\n",
    "        patience = 20,\n",
    "        batch_size = 1024, \n",
    "        virtual_batch_size = 32,\n",
    "        num_workers = 1,\n",
    "        drop_last = False,\n",
    "        loss_fn = BCEWithLogitsLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get prediction (nonscored MoA for test data) for step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_step1 = model.predict(X_test)\n",
    "preds_step1 = 1 / (1 + np.exp(-preds_step1))\n",
    "preds_step1.shape\n",
    "preds_step1 = preds_step1[:, 0:len(train_targets_nonscored.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3624, 194)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_step1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building test features for step 2 (test features+ predicted non-scored MoA's) \n",
    "X_test_step2 = np.concatenate((X_test, preds_step1), axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3624, 1091)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_step2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the next 2 chunks are related to hyper parameter tuning and are commented since they take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the search grid for tuning hyperparameters\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# tabnet_params_2 = dict(\n",
    "#     n_d = [32,64],\n",
    "#     n_steps = [1, 3,5, 9],\n",
    "#     gamma = [1, 1.5, 2],\n",
    "#     lambda_sparse = [0.000001, 0.1],\n",
    "#     optimizer_fn = [optim.Adam],\n",
    "#     optimizer_params = [dict(lr = 0.02, weight_decay = 0.00001)],\n",
    "#     mask_type = [\"entmax\"],\n",
    "#     scheduler_params =[dict(mode = \"min\", patience = 20, min_lr = 1e-5, factor = 0.9)],\n",
    "#     scheduler_fn = [ReduceLROnPlateau],\n",
    "#     verbose = [10])\n",
    "\n",
    "# len(ParameterGrid(tabnet_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 2 hyper parameter tuning\n",
    "# with open('checkpoints.txt', 'w') as chck:\n",
    "#     scores_auc_all = []\n",
    "#     test_cv_preds = []\n",
    "#     MAX_EPOCH = 200\n",
    "#     NB_SPLITS = 4\n",
    "#     mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n",
    "\n",
    "#     oof_preds = []\n",
    "#     oof_targets = []\n",
    "#     # scores = []\n",
    "#     scores_auc = []\n",
    "#     best_score = 9999\n",
    "#     for param in ParameterGrid(tabnet_params_2):\n",
    "#         param['n_a'] = param['n_d']\n",
    "#         print(param)\n",
    "#         param_scores = []\n",
    "#         for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_step2, target_step2)):\n",
    "#             print(b_,\"FOLDS: \", r_, fold_nb + 1)\n",
    "#             print(g_, '*' * 60, c_)\n",
    "\n",
    "#             X_train, y_train = train_step2.values[train_idx, :], target_step2.values[train_idx, :]\n",
    "#             X_val, y_val = train_step2.values[val_idx, :], target_step2.values[val_idx, :]\n",
    "#             ### Model ###\n",
    "#             model = TabNetRegressor(**param)\n",
    "\n",
    "#             ### Fit ###\n",
    "#             model.fit(\n",
    "#                 X_train = X_train,\n",
    "#                 y_train = y_train,\n",
    "#                 eval_set = [(X_val, y_val)],\n",
    "#                 eval_name = [\"val\"],\n",
    "#                 eval_metric = [\"logits_ll\"],\n",
    "#                 max_epochs = MAX_EPOCH,\n",
    "#                 patience = 20,\n",
    "#                 batch_size = 1024, \n",
    "#                 virtual_batch_size = 32,\n",
    "#                 num_workers = 1,\n",
    "#                 drop_last = False,\n",
    "#                 loss_fn = BCEWithLogitsLoss())\n",
    "#             print(y_, '-' * 60)\n",
    "\n",
    "#             ### Predict on validation ###\n",
    "#             preds_val = model.predict(X_val)\n",
    "#             # Apply sigmoid to the predictions\n",
    "#             preds = 1 / (1 + np.exp(-preds_val))\n",
    "#             score = np.min(model.history[\"val_logits_ll\"])\n",
    "\n",
    "#             oof_preds.append(preds_val)\n",
    "#             oof_targets.append(y_val)\n",
    "#             param_scores.append(score)\n",
    "#         current_score = np.mean(param_scores)\n",
    "#         if current_score< best_score:\n",
    "#             print('score was improved from {} to {}'.format(best_score, current_score))\n",
    "#             best_param = param\n",
    "#             chck.write(best_param, +'\\n')\n",
    "#             best_score = current_score\n",
    "#             chck.write(best_score, +'\\n')\n",
    "#             ### Predict on test ###\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the TabNet with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.14937 |  0:00:08s\n",
      "epoch 10 | loss: 0.02022 |  0:01:35s\n",
      "epoch 20 | loss: 0.01893 |  0:03:04s\n",
      "epoch 30 | loss: 0.01753 |  0:04:31s\n",
      "epoch 40 | loss: 0.01653 |  0:05:59s\n",
      "epoch 50 | loss: 0.01581 |  0:07:28s\n",
      "epoch 60 | loss: 0.01563 |  0:08:55s\n",
      "epoch 70 | loss: 0.01547 |  0:10:24s\n",
      "epoch 80 | loss: 0.01552 |  0:12:04s\n",
      "epoch 90 | loss: 0.01542 |  0:13:43s\n"
     ]
    }
   ],
   "source": [
    "model = TabNetRegressor(**best_tabnet_params)\n",
    "model.fit(\n",
    "            X_train = train_step2.values,\n",
    "            y_train = target_step2.values,\n",
    "            max_epochs = MAX_EPOCH,\n",
    "            patience = 20,\n",
    "            batch_size = 1024, \n",
    "            virtual_batch_size = 32,\n",
    "            num_workers = 1,\n",
    "            drop_last = False,\n",
    "            loss_fn = BCEWithLogitsLoss())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the final (scored) columns prediction on test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3624, 206)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = model.predict(X_test_step2)\n",
    "preds_test = 1 / (1 + np.exp(-preds_test))\n",
    "preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aucs = []\n",
    "# for task_id in range(oof_preds_all.shape[1]):\n",
    "#     aucs.append(roc_auc_score(y_true = oof_targets_all[:, task_id],\n",
    "#                               y_score = oof_preds_all[:, task_id]\n",
    "#                              ))\n",
    "# print(f\"{b_}Overall AUC: {r_}{np.mean(aucs)}\")\n",
    "# print(f\"{b_}Average CV: {r_}{np.mean(scores)}\")\n",
    "\n",
    "\n",
    "#without augmentation and chain:\n",
    "# Overall AUC: 0.7530900378764305\n",
    "# Average CV: 0.016472221058818622"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.019551</td>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.001757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.017847</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.012617</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.001848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.000715                0.000204   \n",
       "1  id_001897cda                     0.001066                0.000320   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.000663                0.000231   \n",
       "4  id_0027f1083                     0.001032                0.000287   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.001482                        0.019551   \n",
       "1        0.001595                        0.005118   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.001302                        0.017847   \n",
       "4        0.001394                        0.012617   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.019379                        0.005304   \n",
       "1                           0.002441                        0.001820   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.016440                        0.005537   \n",
       "4                           0.011547                        0.003768   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002348                       0.002891   \n",
       "1                    0.005874                       0.002514   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.002343                       0.003319   \n",
       "4                    0.002589                       0.002745   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000176  ...                               0.000341   \n",
       "1                    0.005877  ...                               0.000420   \n",
       "2                    0.000000  ...                               0.000000   \n",
       "3                    0.000197  ...                               0.000341   \n",
       "4                    0.000360  ...                               0.000375   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.000333         0.004546           0.002004   \n",
       "1      0.000255         0.006475           0.000104   \n",
       "2      0.000000         0.000000           0.000000   \n",
       "3      0.000348         0.004149           0.004139   \n",
       "4      0.000269         0.004460           0.001001   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.002069                               0.000486   \n",
       "1                   0.001380                               0.000547   \n",
       "2                   0.000000                               0.000000   \n",
       "3                   0.003059                               0.000488   \n",
       "4                   0.002316                               0.000557   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.001449   0.001377                    0.001446       0.001757  \n",
       "1         0.009870   0.000963                    0.017641       0.001017  \n",
       "2         0.000000   0.000000                    0.000000       0.000000  \n",
       "3         0.002337   0.001531                    0.000724       0.001887  \n",
       "4         0.004362   0.001484                    0.000692       0.001848  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat = [col for col in df.columns if col not in [\"sig_id\"]]\n",
    "# To obtain the same lenght of test_preds_all and submission\n",
    "test = pd.read_csv(\"test_features.csv\")\n",
    "sig_id = test[test[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\n",
    "tmp = pd.DataFrame(preds_test, columns = all_feat)\n",
    "tmp[\"sig_id\"] = sig_id\n",
    "\n",
    "submission = pd.merge(test[[\"sig_id\"]], tmp, on = \"sig_id\", how = \"left\")\n",
    "submission.fillna(0, inplace = True)\n",
    "submission.to_csv(\"submission.csv\", index = None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
